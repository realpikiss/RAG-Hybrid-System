{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1b4c83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 14:07:14,587 - INFO - Configuration loaded successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  embedding_dimensions: 128\n",
      "  batch_size: 100\n",
      "  cpg_data_path: /Users/vernetemmanueladjobi/Documents/RessourcesStages/Projets/VulRAG-Hybrid-System/data/tmp/cpg_json\n",
      "  kb2_input_path: /Users/vernetemmanueladjobi/Documents/RessourcesStages/Projets/VulRAG-Hybrid-System/data/tmp/kb2_complete.json\n",
      "  kb2_output_path: /Users/vernetemmanueladjobi/Documents/RessourcesStages/Projets/VulRAG-Hybrid-System/data/tmp/kb2_final_with_embeddings.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "KB2 Construction with Graph Embeddings for Vulnerability Detection\n",
    "Scientific implementation for hybrid RAG system\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import logging\n",
    "\n",
    "# Configure logging for scientific reproducibility\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration constants\n",
    "CONFIG = {\n",
    "    'embedding_dimensions': 128,\n",
    "    'batch_size': 100,\n",
    "    'cpg_data_path': Path.home() / \"Documents/RessourcesStages/Projets/VulRAG-Hybrid-System/data/tmp/cpg_json\",\n",
    "    'kb2_input_path': Path.home() / \"Documents/RessourcesStages/Projets/VulRAG-Hybrid-System/data/tmp/kb2_complete.json\",\n",
    "    'kb2_output_path': Path.home() / \"Documents/RessourcesStages/Projets/VulRAG-Hybrid-System/data/tmp/kb2_final_with_embeddings.json\"\n",
    "}\n",
    "\n",
    "logger.info(\"Configuration loaded successfully\")\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fce53542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 14:07:14,747 - INFO - Testing conversion with file: patch_cpg.json\n",
      "2025-06-13 14:07:14,753 - INFO - Conversion successful: {'nodes': 124, 'edges': 574, 'density': 0.07526881720430108, 'connected_components': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphSON Conversion Test Results:\n",
      "  nodes: 124\n",
      "  edges: 574\n",
      "  density: 0.07526881720430108\n",
      "  connected_components: 1\n"
     ]
    }
   ],
   "source": [
    "def graphson_to_networkx(graphson_data: Dict) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Convert GraphSON format CPG to NetworkX graph.\n",
    "    \n",
    "    Args:\n",
    "        graphson_data: Dictionary containing GraphSON formatted CPG data\n",
    "        \n",
    "    Returns:\n",
    "        NetworkX Graph object representing the CPG\n",
    "        \n",
    "    Scientific rationale:\n",
    "        - Preserves graph topology for structural analysis\n",
    "        - Maintains node labels for semantic embedding generation\n",
    "        - Uses undirected graph to focus on connectivity patterns\n",
    "    \"\"\"\n",
    "    vertices = graphson_data['@value']['vertices']\n",
    "    edges = graphson_data['@value']['edges']\n",
    "    \n",
    "    G = nx.Graph()  # Undirected for structural pattern analysis\n",
    "    \n",
    "    # Add vertices with essential attributes\n",
    "    for vertex in vertices:\n",
    "        vertex_id = vertex['id']['@value'] if '@value' in vertex['id'] else vertex['id']\n",
    "        vertex_label = vertex.get('label', 'UNKNOWN')\n",
    "        G.add_node(vertex_id, label=vertex_label)\n",
    "    \n",
    "    # Add edges\n",
    "    for edge in edges:\n",
    "        source = edge['outV']['@value'] if '@value' in edge['outV'] else edge['outV']\n",
    "        target = edge['inV']['@value'] if '@value' in edge['inV'] else edge['inV']\n",
    "        G.add_edge(source, target)\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Test conversion on sample file\n",
    "def test_graphson_conversion() -> Tuple[nx.Graph, Dict]:\n",
    "    \"\"\"Test GraphSON conversion with validation metrics.\"\"\"\n",
    "    \n",
    "    sample_files = list(CONFIG['cpg_data_path'].rglob(\"*.json\"))\n",
    "    if not sample_files:\n",
    "        raise FileNotFoundError(\"No CPG files found in specified directory\")\n",
    "    \n",
    "    test_file = sample_files[0]\n",
    "    logger.info(f\"Testing conversion with file: {test_file.name}\")\n",
    "    \n",
    "    with open(test_file) as f:\n",
    "        graphson_data = json.load(f)\n",
    "    \n",
    "    G = graphson_to_networkx(graphson_data)\n",
    "    \n",
    "    # Validation metrics\n",
    "    metrics = {\n",
    "        'nodes': G.number_of_nodes(),\n",
    "        'edges': G.number_of_edges(),\n",
    "        'density': nx.density(G),\n",
    "        'connected_components': len(list(nx.connected_components(G)))\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"Conversion successful: {metrics}\")\n",
    "    return G, metrics\n",
    "\n",
    "# Execute test\n",
    "test_graph, test_metrics = test_graphson_conversion()\n",
    "print(\"GraphSON Conversion Test Results:\")\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"  {metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "287bfd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing embedding computation:\n",
      "  CVE-2017-7533_0: nodes=124, edges=574, embedding_norm=1.000\n",
      "  CVE-2017-7533_0: nodes=124, edges=571, embedding_norm=1.000\n",
      "  CVE-2021-0935_0: nodes=388, edges=1734, embedding_norm=1.000\n",
      "\n",
      "Pairwise similarities:\n",
      "  CVE-2017-7533_0 <-> CVE-2017-7533_0: 1.000\n",
      "  CVE-2017-7533_0 <-> CVE-2021-0935_0: 0.968\n",
      "  CVE-2017-7533_0 <-> CVE-2021-0935_0: 0.969\n"
     ]
    }
   ],
   "source": [
    "def compute_structural_graph_embedding(G: nx.Graph, dimensions: int = 128) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute structural graph embedding using multi-feature approach.\n",
    "    \n",
    "    Args:\n",
    "        G: NetworkX graph representing CPG\n",
    "        dimensions: Target dimensionality of embedding vector\n",
    "        \n",
    "    Returns:\n",
    "        Normalized embedding vector capturing structural patterns\n",
    "        \n",
    "    Scientific approach:\n",
    "        1. Topological features (connectivity patterns)\n",
    "        2. Node label distribution (semantic structure)\n",
    "        3. Local clustering patterns (code organization)\n",
    "        4. Degree distribution analysis (complexity metrics)\n",
    "        5. L2 normalization for similarity computation\n",
    "    \"\"\"\n",
    "    \n",
    "    if G.number_of_nodes() == 0:\n",
    "        return np.zeros(dimensions, dtype=np.float32)\n",
    "    \n",
    "    features = []\n",
    "    n_nodes = G.number_of_nodes()\n",
    "    n_edges = G.number_of_edges()\n",
    "    \n",
    "    # 1. Global topological features\n",
    "    edge_node_ratio = n_edges / max(n_nodes, 1)\n",
    "    density = nx.density(G)\n",
    "    component_ratio = len(list(nx.connected_components(G))) / max(n_nodes, 1)\n",
    "    \n",
    "    features.extend([edge_node_ratio, density, component_ratio])\n",
    "    \n",
    "    # 2. Node label distribution analysis\n",
    "    label_counts = Counter([G.nodes[n].get('label', 'UNKNOWN') for n in G.nodes()])\n",
    "    total_nodes = max(sum(label_counts.values()), 1)\n",
    "    \n",
    "    # Key CPG node types for vulnerability analysis\n",
    "    vulnerability_relevant_labels = [\n",
    "        'CALL', 'IDENTIFIER', 'CONTROL_STRUCTURE', 'BLOCK',\n",
    "        'LOCAL', 'METHOD_PARAMETER_IN', 'METHOD_PARAMETER_OUT',\n",
    "        'LITERAL', 'RETURN', 'METHOD'\n",
    "    ]\n",
    "    \n",
    "    for label in vulnerability_relevant_labels:\n",
    "        proportion = label_counts.get(label, 0) / total_nodes\n",
    "        features.append(proportion)\n",
    "    \n",
    "    # 3. Degree distribution analysis\n",
    "    degrees = [d for n, d in G.degree()]\n",
    "    if degrees:\n",
    "        deg_mean = np.mean(degrees)\n",
    "        deg_std = np.std(degrees)\n",
    "        deg_range = np.max(degrees) - np.min(degrees)\n",
    "        \n",
    "        # Normalized degree statistics\n",
    "        features.extend([\n",
    "            deg_mean / max(n_nodes, 1),\n",
    "            deg_std / max(deg_mean, 1) if deg_mean > 0 else 0,\n",
    "            deg_range / max(deg_mean, 1) if deg_mean > 0 else 0\n",
    "        ])\n",
    "        \n",
    "        # Degree distribution histogram\n",
    "        degree_hist, _ = np.histogram(degrees, bins=5, density=True)\n",
    "        features.extend(degree_hist.tolist())\n",
    "        \n",
    "        # Hub analysis (high-degree nodes)\n",
    "        high_degree_threshold = np.percentile(degrees, 80)\n",
    "        hub_proportion = sum(1 for d in degrees if d >= high_degree_threshold) / len(degrees)\n",
    "        features.append(hub_proportion)\n",
    "    else:\n",
    "        features.extend([0] * 10)\n",
    "    \n",
    "    # 4. Structural pattern analysis (label connectivity)\n",
    "    label_adjacency_patterns = []\n",
    "    for u, v in G.edges():\n",
    "        label_u = G.nodes[u].get('label', 'UNKNOWN')\n",
    "        label_v = G.nodes[v].get('label', 'UNKNOWN')\n",
    "        pattern = tuple(sorted([label_u, label_v]))\n",
    "        label_adjacency_patterns.append(pattern)\n",
    "    \n",
    "    pattern_counts = Counter(label_adjacency_patterns)\n",
    "    total_edges = max(len(label_adjacency_patterns), 1)\n",
    "    \n",
    "    # Important vulnerability-related patterns\n",
    "    critical_patterns = [\n",
    "        ('CALL', 'IDENTIFIER'), ('CALL', 'CONTROL_STRUCTURE'),\n",
    "        ('CALL', 'LITERAL'), ('IDENTIFIER', 'IDENTIFIER'),\n",
    "        ('BLOCK', 'CALL'), ('CONTROL_STRUCTURE', 'BLOCK')\n",
    "    ]\n",
    "    \n",
    "    for pattern in critical_patterns:\n",
    "        proportion = pattern_counts.get(pattern, 0) / total_edges\n",
    "        features.append(proportion)\n",
    "    \n",
    "    # 5. Clustering analysis by node type\n",
    "    for label in vulnerability_relevant_labels[:5]:  # Top 5 to manage dimensions\n",
    "        nodes_with_label = [n for n in G.nodes() if G.nodes[n].get('label') == label]\n",
    "        if len(nodes_with_label) > 1:\n",
    "            subgraph = G.subgraph(nodes_with_label)\n",
    "            clustering = nx.average_clustering(subgraph) if subgraph.number_of_edges() > 0 else 0\n",
    "        else:\n",
    "            clustering = 0\n",
    "        features.append(clustering)\n",
    "    \n",
    "    # Convert to numpy array and normalize\n",
    "    features = np.array(features, dtype=np.float32)\n",
    "    \n",
    "    # L2 normalization for cosine similarity computation\n",
    "    if np.linalg.norm(features) > 0:\n",
    "        features = features / np.linalg.norm(features)\n",
    "    \n",
    "    # Ensure exact dimensionality\n",
    "    if len(features) < dimensions:\n",
    "        features = np.concatenate([features, np.zeros(dimensions - len(features))])\n",
    "    else:\n",
    "        features = features[:dimensions]\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Test embedding computation\n",
    "def test_embedding_computation() -> None:\n",
    "    \"\"\"Test embedding computation with diversity analysis.\"\"\"\n",
    "    \n",
    "    # Test on multiple files to verify diversity\n",
    "    test_files = list(CONFIG['cpg_data_path'].rglob(\"*.json\"))[:3]\n",
    "    embeddings = []\n",
    "    file_info = []\n",
    "    \n",
    "    print(\"Testing embedding computation:\")\n",
    "    \n",
    "    for cpg_file in test_files:\n",
    "        with open(cpg_file) as f:\n",
    "            graphson_data = json.load(f)\n",
    "        \n",
    "        G = graphson_to_networkx(graphson_data)\n",
    "        embedding = compute_structural_graph_embedding(G, CONFIG['embedding_dimensions'])\n",
    "        \n",
    "        embeddings.append(embedding)\n",
    "        file_info.append({\n",
    "            'file': cpg_file.name,\n",
    "            'instance': cpg_file.parent.name,\n",
    "            'nodes': G.number_of_nodes(),\n",
    "            'edges': G.number_of_edges(),\n",
    "            'embedding_norm': np.linalg.norm(embedding)\n",
    "        })\n",
    "        \n",
    "        print(f\"  {cpg_file.parent.name}: nodes={G.number_of_nodes()}, \"\n",
    "              f\"edges={G.number_of_edges()}, embedding_norm={np.linalg.norm(embedding):.3f}\")\n",
    "    \n",
    "    # Compute pairwise similarities\n",
    "    print(\"\\nPairwise similarities:\")\n",
    "    for i in range(len(embeddings)):\n",
    "        for j in range(i+1, len(embeddings)):\n",
    "            similarity = np.dot(embeddings[i], embeddings[j])\n",
    "            print(f\"  {file_info[i]['instance']} <-> {file_info[j]['instance']}: {similarity:.3f}\")\n",
    "\n",
    "# Execute embedding test\n",
    "test_embedding_computation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e28f881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vulnerability Pattern Analysis:\n",
      "Instance                    | Vuln Nodes | Patch Nodes | Similarity | Structural Change\n",
      "-------------------------------------------------------------------------------------\n",
      "CVE-2017-7533_0           |       124 |        124 |      1.000 |           Minor\n",
      "CVE-2021-0935_0           |       370 |        388 |      1.000 |           Major\n",
      "CVE-2017-14156_0          |        44 |         44 |      1.000 |           Minor\n",
      "CVE-2023-20928_3          |        28 |         40 |      0.995 |           Major\n",
      "CVE-2019-15221_0          |       226 |        248 |      1.000 |           Major\n",
      "CVE-2013-1763_0           |        58 |         69 |      0.999 |           Major\n",
      "CVE-2020-27786_0          |       115 |        131 |      0.999 |           Major\n",
      "CVE-2016-6786_6           |        48 |         39 |      0.997 |           Major\n",
      "CVE-2017-15102_0          |       432 |        432 |      1.000 |           Minor\n",
      "CVE-2023-38430_1          |       313 |        313 |      1.000 |           Minor\n",
      "\n",
      "Statistical Summary:\n",
      "  Mean vuln-patch similarity: 0.999\n",
      "  Standard deviation: 0.002\n",
      "  Range: [0.995, 1.000]\n"
     ]
    }
   ],
   "source": [
    "def analyze_vulnerability_patterns(cpg_dir: Path, max_instances: int = 5) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze structural patterns between vulnerable and patched code versions.\n",
    "    \n",
    "    Args:\n",
    "        cpg_dir: Directory containing CPG files organized by CVE\n",
    "        max_instances: Maximum number of CVE instances to analyze\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing pattern analysis results\n",
    "        \n",
    "    Scientific objective:\n",
    "        Quantify structural differences between vulnerable and patched code\n",
    "        to validate embedding discriminative power for vulnerability detection.\n",
    "    \"\"\"\n",
    "    \n",
    "    analysis_results = {\n",
    "        'instances_analyzed': 0,\n",
    "        'similarity_distributions': [],\n",
    "        'structural_changes': [],\n",
    "        'embedding_statistics': {}\n",
    "    }\n",
    "    \n",
    "    instances_processed = 0\n",
    "    \n",
    "    print(\"Vulnerability Pattern Analysis:\")\n",
    "    print(\"Instance                    | Vuln Nodes | Patch Nodes | Similarity | Structural Change\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    for instance_dir in cpg_dir.iterdir():\n",
    "        if not instance_dir.is_dir() or instances_processed >= max_instances:\n",
    "            continue\n",
    "            \n",
    "        vuln_file = instance_dir / \"vuln_cpg.json\"\n",
    "        patch_file = instance_dir / \"patch_cpg.json\"\n",
    "        \n",
    "        if not (vuln_file.exists() and patch_file.exists()):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Load and process vulnerable version\n",
    "            with open(vuln_file) as f:\n",
    "                vuln_data = json.load(f)\n",
    "            vuln_graph = graphson_to_networkx(vuln_data)\n",
    "            vuln_embedding = compute_structural_graph_embedding(vuln_graph, CONFIG['embedding_dimensions'])\n",
    "            \n",
    "            # Load and process patched version\n",
    "            with open(patch_file) as f:\n",
    "                patch_data = json.load(f)\n",
    "            patch_graph = graphson_to_networkx(patch_data)\n",
    "            patch_embedding = compute_structural_graph_embedding(patch_graph, CONFIG['embedding_dimensions'])\n",
    "            \n",
    "            # Compute similarity\n",
    "            similarity = np.dot(vuln_embedding, patch_embedding)\n",
    "            \n",
    "            # Analyze structural changes\n",
    "            node_change = patch_graph.number_of_nodes() - vuln_graph.number_of_nodes()\n",
    "            edge_change = patch_graph.number_of_edges() - vuln_graph.number_of_edges()\n",
    "            \n",
    "            structural_change_magnitude = abs(node_change) + abs(edge_change)\n",
    "            \n",
    "            # Store results\n",
    "            analysis_results['similarity_distributions'].append(similarity)\n",
    "            analysis_results['structural_changes'].append({\n",
    "                'instance': instance_dir.name,\n",
    "                'node_change': node_change,\n",
    "                'edge_change': edge_change,\n",
    "                'magnitude': structural_change_magnitude,\n",
    "                'similarity': similarity\n",
    "            })\n",
    "            \n",
    "            # Display results\n",
    "            change_indicator = \"Major\" if structural_change_magnitude > 20 else \"Minor\"\n",
    "            print(f\"{instance_dir.name:<25} | {vuln_graph.number_of_nodes():>9} | \"\n",
    "                  f\"{patch_graph.number_of_nodes():>10} | {similarity:>10.3f} | {change_indicator:>15}\")\n",
    "            \n",
    "            instances_processed += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to process {instance_dir.name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    analysis_results['instances_analyzed'] = instances_processed\n",
    "    \n",
    "    # Compute statistics\n",
    "    similarities = analysis_results['similarity_distributions']\n",
    "    if similarities:\n",
    "        analysis_results['embedding_statistics'] = {\n",
    "            'mean_similarity': np.mean(similarities),\n",
    "            'std_similarity': np.std(similarities),\n",
    "            'min_similarity': np.min(similarities),\n",
    "            'max_similarity': np.max(similarities)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nStatistical Summary:\")\n",
    "        print(f\"  Mean vuln-patch similarity: {np.mean(similarities):.3f}\")\n",
    "        print(f\"  Standard deviation: {np.std(similarities):.3f}\")\n",
    "        print(f\"  Range: [{np.min(similarities):.3f}, {np.max(similarities):.3f}]\")\n",
    "    \n",
    "    return analysis_results\n",
    "\n",
    "# Execute vulnerability pattern analysis\n",
    "pattern_analysis = analyze_vulnerability_patterns(CONFIG['cpg_data_path'], max_instances=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a42a87ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing batch processing on 5 entries:\n",
      "  Success: 5/5\n",
      "  Errors: 0/5\n",
      "  Sample embedding for CVE-2017-7533_0_patch:\n",
      "    Shape: (128,)\n",
      "    Norm: 1.000\n",
      "    Non-zero elements: 27\n"
     ]
    }
   ],
   "source": [
    "def process_kb2_batch(entry_batch: List[Tuple[str, Dict]], cpg_dir: Path) -> Tuple[int, int, List[str]]:\n",
    "    \"\"\"\n",
    "    Process a batch of KB2 entries to add graph embeddings.\n",
    "    \n",
    "    Args:\n",
    "        entry_batch: List of (entry_key, entry_data) tuples\n",
    "        cpg_dir: Directory containing CPG files\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (success_count, error_count, error_messages)\n",
    "        \n",
    "    Scientific approach:\n",
    "        Batch processing for memory efficiency and progress tracking\n",
    "        with comprehensive error handling and validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    error_messages = []\n",
    "    \n",
    "    for entry_key, entry_data in entry_batch:\n",
    "        # Skip entries that failed initial extraction\n",
    "        if not entry_data.get('extraction_success', True):\n",
    "            continue\n",
    "            \n",
    "        cve_id = entry_data['cve_id']\n",
    "        file_type = entry_data['file_type']\n",
    "        \n",
    "        # Construct CPG file path\n",
    "        cpg_file = cpg_dir / cve_id / f\"{file_type}_cpg.json\"\n",
    "        \n",
    "        if not cpg_file.exists():\n",
    "            error_message = f\"CPG file not found: {cpg_file}\"\n",
    "            error_messages.append(error_message)\n",
    "            entry_data['embedding_error'] = error_message\n",
    "            error_count += 1\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Load CPG data\n",
    "            with open(cpg_file) as f:\n",
    "                graphson_data = json.load(f)\n",
    "            \n",
    "            # Convert to NetworkX and compute embedding\n",
    "            G = graphson_to_networkx(graphson_data)\n",
    "            embedding = compute_structural_graph_embedding(G, CONFIG['embedding_dimensions'])\n",
    "            \n",
    "            # Validate embedding\n",
    "            if np.isnan(embedding).any() or np.isinf(embedding).any():\n",
    "                raise ValueError(\"Invalid embedding values (NaN or Inf)\")\n",
    "            \n",
    "            # Add embedding and metadata to entry\n",
    "            entry_data['graph_embedding'] = embedding.tolist()\n",
    "            entry_data['embedding_method'] = 'structural_multi_feature'\n",
    "            entry_data['embedding_dimensions'] = CONFIG['embedding_dimensions']\n",
    "            entry_data['graph_statistics'] = {\n",
    "                'nodes': int(G.number_of_nodes()),\n",
    "                'edges': int(G.number_of_edges()),\n",
    "                'density': float(nx.density(G)),\n",
    "                'connected_components': len(list(nx.connected_components(G)))\n",
    "            }\n",
    "            entry_data['embedding_computed'] = True\n",
    "            \n",
    "            success_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_message = f\"Embedding computation failed for {entry_key}: {str(e)}\"\n",
    "            error_messages.append(error_message)\n",
    "            entry_data['embedding_error'] = str(e)\n",
    "            entry_data['embedding_computed'] = False\n",
    "            error_count += 1\n",
    "    \n",
    "    return success_count, error_count, error_messages\n",
    "\n",
    "# Test batch processing on small sample\n",
    "def test_batch_processing() -> None:\n",
    "    \"\"\"Test batch processing functionality with validation.\"\"\"\n",
    "    \n",
    "    # Load KB2 data\n",
    "    with open(CONFIG['kb2_input_path']) as f:\n",
    "        kb2_data = json.load(f)\n",
    "    \n",
    "    # Take small sample for testing\n",
    "    sample_size = 5\n",
    "    sample_items = list(kb2_data.items())[:sample_size]\n",
    "    \n",
    "    print(f\"Testing batch processing on {sample_size} entries:\")\n",
    "    \n",
    "    success, errors, error_msgs = process_kb2_batch(sample_items, CONFIG['cpg_data_path'])\n",
    "    \n",
    "    print(f\"  Success: {success}/{sample_size}\")\n",
    "    print(f\"  Errors: {errors}/{sample_size}\")\n",
    "    \n",
    "    if error_msgs:\n",
    "        print(\"  Error messages:\")\n",
    "        for msg in error_msgs[:3]:  # Show first 3 errors\n",
    "            print(f\"    {msg}\")\n",
    "    \n",
    "    # Validate a successful embedding\n",
    "    for entry_key, entry_data in sample_items:\n",
    "        if entry_data.get('embedding_computed', False):\n",
    "            embedding = np.array(entry_data['graph_embedding'])\n",
    "            print(f\"  Sample embedding for {entry_key}:\")\n",
    "            print(f\"    Shape: {embedding.shape}\")\n",
    "            print(f\"    Norm: {np.linalg.norm(embedding):.3f}\")\n",
    "            print(f\"    Non-zero elements: {np.count_nonzero(embedding)}\")\n",
    "            break\n",
    "\n",
    "# Execute batch processing test\n",
    "test_batch_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9057b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 14:07:15,251 - INFO - Starting complete KB2 processing with embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE KB2 EMBEDDING PROCESSING\n",
      "=============================================\n",
      "Loading KB2 data from: /Users/vernetemmanueladjobi/Documents/RessourcesStages/Projets/VulRAG-Hybrid-System/data/tmp/kb2_complete.json\n",
      "Total KB2 entries to process: 4410\n",
      "Processing in 45 batches of size 100\n",
      "Processing batch 1/45 (entries 1-100)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 2.2%\n",
      "Processing batch 2/45 (entries 101-200)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 4.4%\n",
      "Processing batch 3/45 (entries 201-300)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 6.7%\n",
      "Processing batch 4/45 (entries 301-400)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 8.9%\n",
      "Processing batch 5/45 (entries 401-500)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 11.1%\n",
      "Processing batch 6/45 (entries 501-600)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 13.3%\n",
      "Processing batch 7/45 (entries 601-700)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 15.6%\n",
      "Processing batch 8/45 (entries 701-800)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 17.8%\n",
      "Processing batch 9/45 (entries 801-900)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 20.0%\n",
      "Processing batch 10/45 (entries 901-1000)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 22.2%\n",
      "Processing batch 11/45 (entries 1001-1100)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 24.4%\n",
      "Processing batch 12/45 (entries 1101-1200)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 26.7%\n",
      "Processing batch 13/45 (entries 1201-1300)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 28.9%\n",
      "Processing batch 14/45 (entries 1301-1400)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 31.1%\n",
      "Processing batch 15/45 (entries 1401-1500)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 33.3%\n",
      "Processing batch 16/45 (entries 1501-1600)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 35.6%\n",
      "Processing batch 17/45 (entries 1601-1700)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 37.8%\n",
      "Processing batch 18/45 (entries 1701-1800)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 40.0%\n",
      "Processing batch 19/45 (entries 1801-1900)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 42.2%\n",
      "Processing batch 20/45 (entries 1901-2000)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 44.4%\n",
      "Processing batch 21/45 (entries 2001-2100)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 46.7%\n",
      "Processing batch 22/45 (entries 2101-2200)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 48.9%\n",
      "Processing batch 23/45 (entries 2201-2300)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 51.1%\n",
      "Processing batch 24/45 (entries 2301-2400)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 53.3%\n",
      "Processing batch 25/45 (entries 2401-2500)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 55.6%\n",
      "Processing batch 26/45 (entries 2501-2600)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 57.8%\n",
      "Processing batch 27/45 (entries 2601-2700)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 60.0%\n",
      "Processing batch 28/45 (entries 2701-2800)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 62.2%\n",
      "Processing batch 29/45 (entries 2801-2900)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 64.4%\n",
      "Processing batch 30/45 (entries 2901-3000)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 66.7%\n",
      "Processing batch 31/45 (entries 3001-3100)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 68.9%\n",
      "Processing batch 32/45 (entries 3101-3200)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 71.1%\n",
      "Processing batch 33/45 (entries 3201-3300)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 73.3%\n",
      "Processing batch 34/45 (entries 3301-3400)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 75.6%\n",
      "Processing batch 35/45 (entries 3401-3500)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 77.8%\n",
      "Processing batch 36/45 (entries 3501-3600)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 80.0%\n",
      "Processing batch 37/45 (entries 3601-3700)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 82.2%\n",
      "Processing batch 38/45 (entries 3701-3800)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 84.4%\n",
      "Processing batch 39/45 (entries 3801-3900)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 86.7%\n",
      "Processing batch 40/45 (entries 3901-4000)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 88.9%\n",
      "Processing batch 41/45 (entries 4001-4100)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 91.1%\n",
      "Processing batch 42/45 (entries 4101-4200)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 93.3%\n",
      "Processing batch 43/45 (entries 4201-4300)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 95.6%\n",
      "Processing batch 44/45 (entries 4301-4400)\n",
      "  Batch complete: 100/100 successful (100.0%) | Overall progress: 97.8%\n",
      "Processing batch 45/45 (entries 4401-4410)\n",
      "  Batch complete: 10/10 successful (100.0%) | Overall progress: 100.0%\n",
      "\n",
      "Saving enhanced KB2 to: /Users/vernetemmanueladjobi/Documents/RessourcesStages/Projets/VulRAG-Hybrid-System/data/tmp/kb2_final_with_embeddings.json\n",
      "\n",
      "PROCESSING COMPLETE\n",
      "=========================\n",
      "Total processing time: 52.4 seconds\n",
      "Successful embeddings: 4410\n",
      "Failed embeddings: 0\n",
      "Success rate: 100.0%\n",
      "Output file size: 20.1 MB\n",
      "\n",
      "EMBEDDING QUALITY ANALYSIS\n",
      "Mean embedding norm: 1.000\n",
      "Std embedding norm: 0.000\n",
      "Zero embeddings: 0\n",
      "NaN/Inf embeddings: 0\n"
     ]
    }
   ],
   "source": [
    "def process_complete_kb2_with_embeddings(kb2_input_path: Path, cpg_dir: Path, output_path: Path) -> Dict:\n",
    "    \"\"\"\n",
    "    Process complete KB2 dataset to add graph embeddings with comprehensive logging.\n",
    "    \n",
    "    Args:\n",
    "        kb2_input_path: Path to input KB2 JSON file\n",
    "        cpg_dir: Directory containing CPG files\n",
    "        output_path: Path for output KB2 with embeddings\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing processing statistics and results\n",
    "        \n",
    "    Scientific approach:\n",
    "        - Batch processing for memory efficiency\n",
    "        - Comprehensive error handling and validation\n",
    "        - Progress tracking for large datasets\n",
    "        - Statistical analysis of embedding quality\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"COMPLETE KB2 EMBEDDING PROCESSING\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Load existing KB2 data\n",
    "    print(f\"Loading KB2 data from: {kb2_input_path}\")\n",
    "    with open(kb2_input_path) as f:\n",
    "        kb2_data = json.load(f)\n",
    "    \n",
    "    print(f\"Total KB2 entries to process: {len(kb2_data)}\")\n",
    "    \n",
    "    # Initialize processing statistics\n",
    "    stats = {\n",
    "        'total_entries': len(kb2_data),\n",
    "        'successful_embeddings': 0,\n",
    "        'failed_embeddings': 0,\n",
    "        'processing_errors': [],\n",
    "        'embedding_statistics': {\n",
    "            'mean_norm': 0.0,\n",
    "            'std_norm': 0.0,\n",
    "            'zero_embeddings': 0,\n",
    "            'nan_embeddings': 0\n",
    "        },\n",
    "        'processing_time': 0,\n",
    "        'batch_results': []\n",
    "    }\n",
    "    \n",
    "    # Convert to list for batch processing\n",
    "    entry_items = list(kb2_data.items())\n",
    "    batch_size = CONFIG['batch_size']\n",
    "    total_batches = (len(entry_items) + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"Processing in {total_batches} batches of size {batch_size}\")\n",
    "    \n",
    "    # Track embedding norms for quality analysis\n",
    "    embedding_norms = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process in batches with progress tracking\n",
    "    for batch_idx in range(total_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, len(entry_items))\n",
    "        batch = entry_items[start_idx:end_idx]\n",
    "        \n",
    "        print(f\"Processing batch {batch_idx + 1}/{total_batches} \"\n",
    "              f\"(entries {start_idx + 1}-{end_idx})\")\n",
    "        \n",
    "        # Process current batch\n",
    "        batch_success, batch_errors, batch_error_msgs = process_kb2_batch(batch, cpg_dir)\n",
    "        \n",
    "        # Update statistics\n",
    "        stats['successful_embeddings'] += batch_success\n",
    "        stats['failed_embeddings'] += batch_errors\n",
    "        stats['processing_errors'].extend(batch_error_msgs)\n",
    "        \n",
    "        # Collect embedding norms for quality analysis\n",
    "        for entry_key, entry_data in batch:\n",
    "            if entry_data.get('embedding_computed', False):\n",
    "                embedding = np.array(entry_data['graph_embedding'])\n",
    "                norm = np.linalg.norm(embedding)\n",
    "                embedding_norms.append(norm)\n",
    "                \n",
    "                # Check for problematic embeddings\n",
    "                if norm == 0:\n",
    "                    stats['embedding_statistics']['zero_embeddings'] += 1\n",
    "                if np.isnan(embedding).any() or np.isinf(embedding).any():\n",
    "                    stats['embedding_statistics']['nan_embeddings'] += 1\n",
    "        \n",
    "        # Store batch results\n",
    "        stats['batch_results'].append({\n",
    "            'batch_idx': batch_idx,\n",
    "            'success_count': batch_success,\n",
    "            'error_count': batch_errors,\n",
    "            'success_rate': batch_success / len(batch) if len(batch) > 0 else 0\n",
    "        })\n",
    "        \n",
    "        # Progress update\n",
    "        overall_progress = (batch_idx + 1) / total_batches * 100\n",
    "        print(f\"  Batch complete: {batch_success}/{len(batch)} successful \"\n",
    "              f\"({batch_success/len(batch)*100:.1f}%) | \"\n",
    "              f\"Overall progress: {overall_progress:.1f}%\")\n",
    "    \n",
    "    # Calculate final statistics\n",
    "    processing_time = time.time() - start_time\n",
    "    stats['processing_time'] = processing_time\n",
    "    \n",
    "    if embedding_norms:\n",
    "        stats['embedding_statistics']['mean_norm'] = float(np.mean(embedding_norms))\n",
    "        stats['embedding_statistics']['std_norm'] = float(np.std(embedding_norms))\n",
    "    \n",
    "    # Save processed KB2 data\n",
    "    print(f\"\\nSaving enhanced KB2 to: {output_path}\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(kb2_data, f, indent=2)\n",
    "    \n",
    "    # Generate processing report\n",
    "    print(f\"\\nPROCESSING COMPLETE\")\n",
    "    print(f\"=\" * 25)\n",
    "    print(f\"Total processing time: {processing_time:.1f} seconds\")\n",
    "    print(f\"Successful embeddings: {stats['successful_embeddings']}\")\n",
    "    print(f\"Failed embeddings: {stats['failed_embeddings']}\")\n",
    "    print(f\"Success rate: {stats['successful_embeddings']/stats['total_entries']*100:.1f}%\")\n",
    "    print(f\"Output file size: {output_path.stat().st_size / (1024*1024):.1f} MB\")\n",
    "    \n",
    "    if embedding_norms:\n",
    "        print(f\"\\nEMBEDDING QUALITY ANALYSIS\")\n",
    "        print(f\"Mean embedding norm: {stats['embedding_statistics']['mean_norm']:.3f}\")\n",
    "        print(f\"Std embedding norm: {stats['embedding_statistics']['std_norm']:.3f}\")\n",
    "        print(f\"Zero embeddings: {stats['embedding_statistics']['zero_embeddings']}\")\n",
    "        print(f\"NaN/Inf embeddings: {stats['embedding_statistics']['nan_embeddings']}\")\n",
    "    \n",
    "    if stats['processing_errors']:\n",
    "        print(f\"\\nFirst 5 processing errors:\")\n",
    "        for i, error in enumerate(stats['processing_errors'][:5]):\n",
    "            print(f\"  {i+1}. {error}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Import required modules\n",
    "import time\n",
    "\n",
    "# Execute complete processing\n",
    "logger.info(\"Starting complete KB2 processing with embeddings\")\n",
    "processing_stats = process_complete_kb2_with_embeddings(\n",
    "    CONFIG['kb2_input_path'], \n",
    "    CONFIG['cpg_data_path'], \n",
    "    CONFIG['kb2_output_path']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42e98cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 14:08:08,259 - INFO - KB2 Search Engine initialized with 4410 entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING KB2 SEARCH ENGINE\n",
      "==============================\n",
      "Loading KB2 data from: /Users/vernetemmanueladjobi/Documents/RessourcesStages/Projets/VulRAG-Hybrid-System/data/tmp/kb2_final_with_embeddings.json\n",
      "Loaded 4410 entries with valid embeddings out of 4410 total\n",
      "Built embedding matrix: (4410, 128)\n",
      "Embedding dimension: 128\n",
      "Memory usage: 2.2 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 14:08:08,373 - INFO - No candidates found above similarity threshold 0.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Statistics:\n",
      "  total_embeddings: 4410\n",
      "  embedding_dimension: 128\n",
      "  norm_statistics:\n",
      "    mean: 1.000\n",
      "    std: 0.000\n",
      "    min: 1.000\n",
      "    max: 1.000\n",
      "  similarity_statistics:\n",
      "    mean_pairwise_similarity: 0.844\n",
      "    std_pairwise_similarity: 0.274\n",
      "    max_pairwise_similarity: 1.000\n",
      "    min_pairwise_similarity: 0.108\n",
      "\n",
      "Testing similarity search...\n",
      "Found 5 similar entries:\n",
      "  1. CVE-2021-43057_1_patch\n",
      "     CVE: CVE-2021-43057_1\n",
      "     Type: patch\n",
      "     Final similarity: 0.190\n",
      "     Embedding sim: 0.199\n",
      "     Feature sim: 0.169\n",
      "     Dangerous calls: []\n",
      "\n",
      "  2. CVE-2021-43057_1_vuln\n",
      "     CVE: CVE-2021-43057_1\n",
      "     Type: vuln\n",
      "     Final similarity: 0.190\n",
      "     Embedding sim: 0.199\n",
      "     Feature sim: 0.169\n",
      "     Dangerous calls: []\n",
      "\n",
      "  3. CVE-2019-19252_0_vuln\n",
      "     CVE: CVE-2019-19252_0\n",
      "     Type: vuln\n",
      "     Final similarity: 0.156\n",
      "     Embedding sim: 0.201\n",
      "     Feature sim: 0.051\n",
      "     Dangerous calls: []\n",
      "\n",
      "  4. CVE-2016-10088_0_patch\n",
      "     CVE: CVE-2016-10088_0\n",
      "     Type: patch\n",
      "     Final similarity: 0.156\n",
      "     Embedding sim: 0.201\n",
      "     Feature sim: 0.051\n",
      "     Dangerous calls: []\n",
      "\n",
      "  5. CVE-2016-10088_0_vuln\n",
      "     CVE: CVE-2016-10088_0\n",
      "     Type: vuln\n",
      "     Final similarity: 0.156\n",
      "     Embedding sim: 0.201\n",
      "     Feature sim: 0.051\n",
      "     Dangerous calls: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class KB2SimilaritySearchEngine:\n",
    "    \"\"\"\n",
    "    Efficient similarity search engine for KB2 structural embeddings.\n",
    "    \n",
    "    Scientific approach:\n",
    "        - Cosine similarity for normalized embeddings\n",
    "        - Hybrid scoring combining structural and feature-based similarity\n",
    "        - Configurable similarity thresholds\n",
    "        - Performance optimization for large datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, kb2_path: Path, similarity_threshold: float = 0.7):\n",
    "        \"\"\"\n",
    "        Initialize search engine with KB2 data.\n",
    "        \n",
    "        Args:\n",
    "            kb2_path: Path to KB2 file with embeddings\n",
    "            similarity_threshold: Minimum similarity for candidate selection\n",
    "        \"\"\"\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.kb2_data = {}\n",
    "        self.embeddings_matrix = None\n",
    "        self.entry_keys = []\n",
    "        \n",
    "        self._load_kb2_data(kb2_path)\n",
    "        self._build_embedding_matrix()\n",
    "        \n",
    "        logger.info(f\"KB2 Search Engine initialized with {len(self.kb2_data)} entries\")\n",
    "    \n",
    "    def _load_kb2_data(self, kb2_path: Path) -> None:\n",
    "        \"\"\"Load KB2 data and filter entries with valid embeddings.\"\"\"\n",
    "        \n",
    "        print(f\"Loading KB2 data from: {kb2_path}\")\n",
    "        with open(kb2_path) as f:\n",
    "            raw_data = json.load(f)\n",
    "        \n",
    "        # Filter entries with valid embeddings\n",
    "        valid_entries = 0\n",
    "        for key, entry in raw_data.items():\n",
    "            if (entry.get('embedding_computed', False) and \n",
    "                'graph_embedding' in entry and \n",
    "                entry['graph_embedding'] is not None):\n",
    "                self.kb2_data[key] = entry\n",
    "                valid_entries += 1\n",
    "        \n",
    "        print(f\"Loaded {valid_entries} entries with valid embeddings out of {len(raw_data)} total\")\n",
    "    \n",
    "    def _build_embedding_matrix(self) -> None:\n",
    "        \"\"\"Build numpy matrix of embeddings for efficient similarity computation.\"\"\"\n",
    "        \n",
    "        if not self.kb2_data:\n",
    "            raise ValueError(\"No valid KB2 data loaded\")\n",
    "        \n",
    "        self.entry_keys = list(self.kb2_data.keys())\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for key in self.entry_keys:\n",
    "            embedding = np.array(self.kb2_data[key]['graph_embedding'], dtype=np.float32)\n",
    "            embeddings_list.append(embedding)\n",
    "        \n",
    "        self.embeddings_matrix = np.vstack(embeddings_list)\n",
    "        \n",
    "        print(f\"Built embedding matrix: {self.embeddings_matrix.shape}\")\n",
    "        print(f\"Embedding dimension: {self.embeddings_matrix.shape[1]}\")\n",
    "        print(f\"Memory usage: {self.embeddings_matrix.nbytes / (1024*1024):.1f} MB\")\n",
    "    \n",
    "    def compute_structural_features_similarity(self, query_features: Dict, candidate_key: str) -> float:\n",
    "        \"\"\"\n",
    "        Compute similarity based on structural features (non-embedding).\n",
    "        \n",
    "        Args:\n",
    "            query_features: Dictionary of query structural features\n",
    "            candidate_key: Key of candidate entry in KB2\n",
    "            \n",
    "        Returns:\n",
    "            Feature-based similarity score [0, 1]\n",
    "        \"\"\"\n",
    "        \n",
    "        candidate_entry = self.kb2_data[candidate_key]\n",
    "        candidate_features = candidate_entry['features']\n",
    "        \n",
    "        # Initialize similarity components\n",
    "        similarities = []\n",
    "        \n",
    "        # 1. Dangerous calls similarity (Jaccard)\n",
    "        query_dangerous = set(query_features.get('dangerous_calls', []))\n",
    "        candidate_dangerous = set(candidate_features['security_features']['dangerous_calls'].keys())\n",
    "        \n",
    "        if query_dangerous or candidate_dangerous:\n",
    "            jaccard_dangerous = len(query_dangerous & candidate_dangerous) / max(len(query_dangerous | candidate_dangerous), 1)\n",
    "            similarities.append(('dangerous_calls', jaccard_dangerous, 0.3))\n",
    "        \n",
    "        # 2. Complexity similarity (normalized difference)\n",
    "        query_complexity = query_features.get('complexity_score', 0)\n",
    "        candidate_complexity = candidate_features['complexity_metrics']['edge_density']\n",
    "        \n",
    "        if max(query_complexity, candidate_complexity) > 0:\n",
    "            complexity_sim = 1 - abs(query_complexity - candidate_complexity) / max(query_complexity, candidate_complexity)\n",
    "            similarities.append(('complexity', complexity_sim, 0.2))\n",
    "        \n",
    "        # 3. Function calls similarity (top calls overlap)\n",
    "        query_calls = set(query_features.get('top_calls', []))\n",
    "        candidate_calls = set(list(candidate_features['code_patterns']['all_calls'].keys())[:10])\n",
    "        \n",
    "        if query_calls or candidate_calls:\n",
    "            calls_jaccard = len(query_calls & candidate_calls) / max(len(query_calls | candidate_calls), 1)\n",
    "            similarities.append(('function_calls', calls_jaccard, 0.3))\n",
    "        \n",
    "        # 4. Vertex type distribution similarity\n",
    "        query_types = query_features.get('vertex_types', {})\n",
    "        candidate_types = candidate_features['code_patterns']['vertex_type_distribution']\n",
    "        \n",
    "        if query_types and candidate_types:\n",
    "            # Compute cosine similarity of type distributions\n",
    "            all_types = set(query_types.keys()) | set(candidate_types.keys())\n",
    "            query_vec = np.array([query_types.get(t, 0) for t in all_types])\n",
    "            candidate_vec = np.array([candidate_types.get(t, 0) for t in all_types])\n",
    "            \n",
    "            if np.linalg.norm(query_vec) > 0 and np.linalg.norm(candidate_vec) > 0:\n",
    "                types_sim = np.dot(query_vec, candidate_vec) / (np.linalg.norm(query_vec) * np.linalg.norm(candidate_vec))\n",
    "                similarities.append(('vertex_types', types_sim, 0.2))\n",
    "        \n",
    "        # Compute weighted average\n",
    "        if similarities:\n",
    "            total_weight = sum(weight for _, _, weight in similarities)\n",
    "            weighted_sim = sum(sim * weight for _, sim, weight in similarities) / total_weight\n",
    "            return weighted_sim\n",
    "        else:\n",
    "            return 0.0\n",
    "    \n",
    "    def search_similar_graphs(self, query_embedding: np.ndarray, \n",
    "                            query_features: Dict = None, \n",
    "                            top_k: int = 10, \n",
    "                            hybrid_weight: float = 0.7) -> List[Tuple[str, float, Dict]]:\n",
    "        \"\"\"\n",
    "        Search for similar graphs using hybrid similarity.\n",
    "        \n",
    "        Args:\n",
    "            query_embedding: Query graph embedding vector\n",
    "            query_features: Optional structural features for hybrid scoring\n",
    "            top_k: Number of top candidates to return\n",
    "            hybrid_weight: Weight for embedding similarity vs feature similarity\n",
    "            \n",
    "        Returns:\n",
    "            List of (entry_key, similarity_score, metadata) tuples\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.embeddings_matrix is None:\n",
    "            raise ValueError(\"Embedding matrix not built\")\n",
    "        \n",
    "        # Normalize query embedding\n",
    "        query_norm = np.linalg.norm(query_embedding)\n",
    "        if query_norm == 0:\n",
    "            logger.warning(\"Query embedding has zero norm\")\n",
    "            return []\n",
    "        \n",
    "        normalized_query = query_embedding / query_norm\n",
    "        \n",
    "        # Compute cosine similarities with all embeddings\n",
    "        cosine_similarities = np.dot(self.embeddings_matrix, normalized_query)\n",
    "        \n",
    "        # Get initial candidates above threshold\n",
    "        candidate_indices = np.where(cosine_similarities >= self.similarity_threshold)[0]\n",
    "        \n",
    "        if len(candidate_indices) == 0:\n",
    "            logger.info(f\"No candidates found above similarity threshold {self.similarity_threshold}\")\n",
    "            # Lower threshold and take top candidates\n",
    "            candidate_indices = np.argsort(cosine_similarities)[-min(top_k*2, len(cosine_similarities)):]\n",
    "        \n",
    "        # Compute hybrid scores if features provided\n",
    "        results = []\n",
    "        for idx in candidate_indices:\n",
    "            entry_key = self.entry_keys[idx]\n",
    "            embedding_sim = float(cosine_similarities[idx])\n",
    "            \n",
    "            # Compute final similarity score\n",
    "            if query_features is not None:\n",
    "                feature_sim = self.compute_structural_features_similarity(query_features, entry_key)\n",
    "                final_sim = hybrid_weight * embedding_sim + (1 - hybrid_weight) * feature_sim\n",
    "            else:\n",
    "                final_sim = embedding_sim\n",
    "                feature_sim = 0.0\n",
    "            \n",
    "            # Collect metadata\n",
    "            entry_data = self.kb2_data[entry_key]\n",
    "            metadata = {\n",
    "                'cve_id': entry_data['cve_id'],\n",
    "                'file_type': entry_data['file_type'],\n",
    "                'embedding_similarity': embedding_sim,\n",
    "                'feature_similarity': feature_sim,\n",
    "                'final_similarity': final_sim,\n",
    "                'graph_stats': entry_data.get('graph_statistics', {}),\n",
    "                'dangerous_calls': list(entry_data['features']['security_features']['dangerous_calls'].keys())\n",
    "            }\n",
    "            \n",
    "            results.append((entry_key, final_sim, metadata))\n",
    "        \n",
    "        # Sort by final similarity and return top-k\n",
    "        results.sort(key=lambda x: x[1], reverse=True)\n",
    "        return results[:top_k]\n",
    "    \n",
    "    def get_embedding_statistics(self) -> Dict:\n",
    "        \"\"\"Get statistics about the embedding matrix for analysis.\"\"\"\n",
    "        \n",
    "        if self.embeddings_matrix is None:\n",
    "            return {}\n",
    "        \n",
    "        norms = np.linalg.norm(self.embeddings_matrix, axis=1)\n",
    "        pairwise_sims = np.dot(self.embeddings_matrix, self.embeddings_matrix.T)\n",
    "        \n",
    "        # Remove diagonal (self-similarities)\n",
    "        mask = ~np.eye(pairwise_sims.shape[0], dtype=bool)\n",
    "        off_diagonal_sims = pairwise_sims[mask]\n",
    "        \n",
    "        return {\n",
    "            'total_embeddings': self.embeddings_matrix.shape[0],\n",
    "            'embedding_dimension': self.embeddings_matrix.shape[1],\n",
    "            'norm_statistics': {\n",
    "                'mean': float(np.mean(norms)),\n",
    "                'std': float(np.std(norms)),\n",
    "                'min': float(np.min(norms)),\n",
    "                'max': float(np.max(norms))\n",
    "            },\n",
    "            'similarity_statistics': {\n",
    "                'mean_pairwise_similarity': float(np.mean(off_diagonal_sims)),\n",
    "                'std_pairwise_similarity': float(np.std(off_diagonal_sims)),\n",
    "                'max_pairwise_similarity': float(np.max(off_diagonal_sims)),\n",
    "                'min_pairwise_similarity': float(np.min(off_diagonal_sims))\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Test the search engine\n",
    "def test_kb2_search_engine():\n",
    "    \"\"\"Test KB2 search engine functionality.\"\"\"\n",
    "    \n",
    "    print(\"TESTING KB2 SEARCH ENGINE\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Initialize search engine\n",
    "    search_engine = KB2SimilaritySearchEngine(CONFIG['kb2_output_path'])\n",
    "    \n",
    "    # Get embedding statistics\n",
    "    stats = search_engine.get_embedding_statistics()\n",
    "    print(\"Embedding Statistics:\")\n",
    "    for category, values in stats.items():\n",
    "        if isinstance(values, dict):\n",
    "            print(f\"  {category}:\")\n",
    "            for key, value in values.items():\n",
    "                print(f\"    {key}: {value:.3f}\" if isinstance(value, float) else f\"    {key}: {value}\")\n",
    "        else:\n",
    "            print(f\"  {category}: {values}\")\n",
    "    \n",
    "    # Test search with a random embedding\n",
    "    print(f\"\\nTesting similarity search...\")\n",
    "    test_embedding = np.random.randn(CONFIG['embedding_dimensions'])\n",
    "    test_embedding = test_embedding / np.linalg.norm(test_embedding)  # Normalize\n",
    "    \n",
    "    # Test features for hybrid search\n",
    "    test_features = {\n",
    "        'dangerous_calls': ['malloc', 'strcpy'],\n",
    "        'complexity_score': 0.5,\n",
    "        'top_calls': ['malloc', 'free', 'strcpy'],\n",
    "        'vertex_types': {'CALL': 10, 'IDENTIFIER': 20, 'BLOCK': 5}\n",
    "    }\n",
    "    \n",
    "    # Perform search\n",
    "    results = search_engine.search_similar_graphs(\n",
    "        test_embedding, \n",
    "        test_features, \n",
    "        top_k=5, \n",
    "        hybrid_weight=0.7\n",
    "    )\n",
    "    \n",
    "    print(f\"Found {len(results)} similar entries:\")\n",
    "    for i, (entry_key, similarity, metadata) in enumerate(results):\n",
    "        print(f\"  {i+1}. {entry_key}\")\n",
    "        print(f\"     CVE: {metadata['cve_id']}\")\n",
    "        print(f\"     Type: {metadata['file_type']}\")\n",
    "        print(f\"     Final similarity: {similarity:.3f}\")\n",
    "        print(f\"     Embedding sim: {metadata['embedding_similarity']:.3f}\")\n",
    "        print(f\"     Feature sim: {metadata['feature_similarity']:.3f}\")\n",
    "        print(f\"     Dangerous calls: {metadata['dangerous_calls']}\")\n",
    "        print()\n",
    "    \n",
    "    return search_engine\n",
    "\n",
    "# Execute test\n",
    "kb2_search_engine = test_kb2_search_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d26f4fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction manquante depuis notebook 03\n",
    "def analyze_full_cpg(cpg_file):\n",
    "    \"\"\"Analyze full CPG file for features extraction.\"\"\"\n",
    "    \n",
    "    with open(cpg_file) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    vertices = data['@value']['vertices']\n",
    "    edges = data['@value']['edges']\n",
    "    \n",
    "    analysis = {\n",
    "        'dangerous_calls': {},\n",
    "        'all_calls': {},\n",
    "        'vertex_types': {},\n",
    "        'file_info': {\n",
    "            'vertex_count': len(vertices),\n",
    "            'edge_count': len(edges)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Extract dangerous calls and vertex types\n",
    "    dangerous_functions = ['strcpy', 'strcat', 'sprintf', 'malloc', 'free', 'memcpy', 'memset']\n",
    "    \n",
    "    for vertex in vertices:\n",
    "        label = vertex.get('label', 'UNKNOWN')\n",
    "        analysis['vertex_types'][label] = analysis['vertex_types'].get(label, 0) + 1\n",
    "        \n",
    "        if label == 'CALL':\n",
    "            # Extract call name\n",
    "            props = vertex.get('properties', {})\n",
    "            if 'NAME' in props:\n",
    "                name_prop = props['NAME']\n",
    "                if isinstance(name_prop, dict) and '@value' in name_prop:\n",
    "                    value_data = name_prop['@value']\n",
    "                    if isinstance(value_data, dict) and '@value' in value_data:\n",
    "                        name = value_data['@value'][0] if isinstance(value_data['@value'], list) else value_data['@value']\n",
    "                        \n",
    "                        analysis['all_calls'][name] = analysis['all_calls'].get(name, 0) + 1\n",
    "                        \n",
    "                        # Check if dangerous\n",
    "                        for dangerous in dangerous_functions:\n",
    "                            if dangerous in str(name).lower():\n",
    "                                analysis['dangerous_calls'][name] = analysis['dangerous_calls'].get(name, 0) + 1\n",
    "    \n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d0da222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXECUTING COMPREHENSIVE KB2 TESTING\n",
      "=============================================\n",
      "REAL CPG QUERY PIPELINE TESTING\n",
      "========================================\n",
      "Testing with 5 CVE instances\n",
      "\n",
      "Testing CVE 1: CVE-2017-7533_0\n",
      "------------------------------\n",
      "Processing vulnerable version...\n",
      "  Query graph: 124 nodes, 571 edges\n",
      "  Dangerous calls: ['fsnotify_oldname_free']\n",
      "  Found 10 similar entries\n",
      "     1. CVE-2017-7533_0_vuln      | Sim: 0.930 | CVE: CVE-2017-7533_0 | Type: vuln\n",
      "     2. CVE-2017-7533_1_vuln      | Sim: 0.844 | CVE: CVE-2017-7533_1 | Type: vuln\n",
      "     3. CVE-2017-7374_0_patch     | Sim: 0.822 | CVE: CVE-2017-7374_0 | Type: patch\n",
      "     4. CVE-2017-7374_4_patch     | Sim: 0.822 | CVE: CVE-2017-7374_4 | Type: patch\n",
      "     5. CVE-2017-7533_0_patch     | Sim: 0.821 | CVE: CVE-2017-7533_0 | Type: patch\n",
      "     6. CVE-2021-0941_1_vuln      | Sim: 0.819 | CVE: CVE-2021-0941_1 | Type: vuln\n",
      "     7. CVE-2016-10147_0_patch    | Sim: 0.819 | CVE: CVE-2016-10147_0 | Type: patch\n",
      "     8. CVE-2016-10147_0_vuln     | Sim: 0.818 | CVE: CVE-2016-10147_0 | Type: vuln\n",
      "     9. CVE-2017-7374_0_vuln      | Sim: 0.817 | CVE: CVE-2017-7374_0 | Type: vuln\n",
      "    10. CVE-2017-7374_4_vuln      | Sim: 0.817 | CVE: CVE-2017-7374_4 | Type: vuln\n",
      "  Patch rank in results: 5\n",
      "  Vuln-patch similarity: 1.000\n",
      "  Same CVE precision: 0.200\n",
      "\n",
      "Testing CVE 2: CVE-2021-0935_0\n",
      "------------------------------\n",
      "Processing vulnerable version...\n",
      "  Query graph: 370 nodes, 1639 edges\n",
      "  Dangerous calls: ['memset']\n",
      "  Found 10 similar entries\n",
      "     1. CVE-2021-0935_0_vuln      | Sim: 0.922 | CVE: CVE-2021-0935_0 | Type: vuln\n",
      "     2. CVE-2017-16534_0_vuln     | Sim: 0.918 | CVE: CVE-2017-16534_0 | Type: vuln\n",
      "     3. CVE-2017-16534_0_patch    | Sim: 0.918 | CVE: CVE-2017-16534_0 | Type: patch\n",
      "     4. CVE-2016-4997_0_vuln      | Sim: 0.913 | CVE: CVE-2016-4997_0 | Type: vuln\n",
      "     5. CVE-2016-4997_1_vuln      | Sim: 0.913 | CVE: CVE-2016-4997_1 | Type: vuln\n",
      "     6. CVE-2016-4997_0_patch     | Sim: 0.913 | CVE: CVE-2016-4997_0 | Type: patch\n",
      "     7. CVE-2016-4997_1_patch     | Sim: 0.913 | CVE: CVE-2016-4997_1 | Type: patch\n",
      "     8. CVE-2016-4998_1_vuln      | Sim: 0.913 | CVE: CVE-2016-4998_1 | Type: vuln\n",
      "     9. CVE-2016-4998_1_patch     | Sim: 0.913 | CVE: CVE-2016-4998_1 | Type: patch\n",
      "    10. CVE-2018-7755_0_patch     | Sim: 0.913 | CVE: CVE-2018-7755_0 | Type: patch\n",
      "  Patch rank in results: Not found\n",
      "  Vuln-patch similarity: 1.000\n",
      "  Same CVE precision: 0.100\n",
      "\n",
      "Testing CVE 3: CVE-2017-14156_0\n",
      "------------------------------\n",
      "Processing vulnerable version...\n",
      "  Query graph: 44 nodes, 125 edges\n",
      "  Dangerous calls: []\n",
      "  Found 10 similar entries\n",
      "     1. CVE-2017-14156_0_patch    | Sim: 0.959 | CVE: CVE-2017-14156_0 | Type: patch\n",
      "     2. CVE-2017-14156_0_vuln     | Sim: 0.959 | CVE: CVE-2017-14156_0 | Type: vuln\n",
      "     3. CVE-2015-7884_0_vuln      | Sim: 0.878 | CVE: CVE-2015-7884_0 | Type: vuln\n",
      "     4. CVE-2021-27365_3_patch    | Sim: 0.869 | CVE: CVE-2021-27365_3 | Type: patch\n",
      "     5. CVE-2021-27365_0_patch    | Sim: 0.867 | CVE: CVE-2021-27365_0 | Type: patch\n",
      "     6. CVE-2018-12207_0_vuln     | Sim: 0.865 | CVE: CVE-2018-12207_0 | Type: vuln\n",
      "     7. CVE-2016-9755_0_vuln      | Sim: 0.864 | CVE: CVE-2016-9755_0 | Type: vuln\n",
      "     8. CVE-2022-26878_0_vuln     | Sim: 0.864 | CVE: CVE-2022-26878_0 | Type: vuln\n",
      "     9. CVE-2019-11487_4_patch    | Sim: 0.861 | CVE: CVE-2019-11487_4 | Type: patch\n",
      "    10. CVE-2021-27365_1_patch    | Sim: 0.860 | CVE: CVE-2021-27365_1 | Type: patch\n",
      "  Patch rank in results: 1\n",
      "  Vuln-patch similarity: 1.000\n",
      "  Same CVE precision: 0.200\n",
      "\n",
      "Testing CVE 4: CVE-2023-20928_3\n",
      "------------------------------\n",
      "Processing vulnerable version...\n",
      "  Query graph: 28 nodes, 84 edges\n",
      "  Dangerous calls: []\n",
      "  Found 10 similar entries\n",
      "     1. CVE-2023-20928_3_vuln     | Sim: 0.959 | CVE: CVE-2023-20928_3 | Type: vuln\n",
      "     2. CVE-2017-2647_2_patch     | Sim: 0.903 | CVE: CVE-2017-2647_2 | Type: patch\n",
      "     3. CVE-2017-2647_2_vuln      | Sim: 0.901 | CVE: CVE-2017-2647_2 | Type: vuln\n",
      "     4. CVE-2016-6828_0_vuln      | Sim: 0.895 | CVE: CVE-2016-6828_0 | Type: vuln\n",
      "     5. CVE-2021-29266_0_patch    | Sim: 0.891 | CVE: CVE-2021-29266_0 | Type: patch\n",
      "     6. CVE-2021-29649_1_patch    | Sim: 0.889 | CVE: CVE-2021-29649_1 | Type: patch\n",
      "     7. CVE-2020-12657_0_vuln     | Sim: 0.885 | CVE: CVE-2020-12657_0 | Type: vuln\n",
      "     8. CVE-2023-20928_3_patch    | Sim: 0.884 | CVE: CVE-2023-20928_3 | Type: patch\n",
      "     9. CVE-2023-20928_1_vuln     | Sim: 0.883 | CVE: CVE-2023-20928_1 | Type: vuln\n",
      "    10. CVE-2023-32233_0_vuln     | Sim: 0.882 | CVE: CVE-2023-32233_0 | Type: vuln\n",
      "  Patch rank in results: 8\n",
      "  Vuln-patch similarity: 0.995\n",
      "  Same CVE precision: 0.200\n",
      "\n",
      "Testing CVE 5: CVE-2019-15221_0\n",
      "------------------------------\n",
      "Processing vulnerable version...\n",
      "  Query graph: 226 nodes, 980 edges\n",
      "  Dangerous calls: []\n",
      "  Found 10 similar entries\n",
      "     1. CVE-2021-20292_0_vuln     | Sim: 0.887 | CVE: CVE-2021-20292_0 | Type: vuln\n",
      "     2. CVE-2020-11668_1_patch    | Sim: 0.882 | CVE: CVE-2020-11668_1 | Type: patch\n",
      "     3. CVE-2018-10074_0_patch    | Sim: 0.880 | CVE: CVE-2018-10074_0 | Type: patch\n",
      "     4. CVE-2018-10074_0_vuln     | Sim: 0.879 | CVE: CVE-2018-10074_0 | Type: vuln\n",
      "     5. CVE-2020-11668_2_patch    | Sim: 0.878 | CVE: CVE-2020-11668_2 | Type: patch\n",
      "     6. CVE-2020-11668_2_vuln     | Sim: 0.878 | CVE: CVE-2020-11668_2 | Type: vuln\n",
      "     7. CVE-2015-9289_0_patch     | Sim: 0.877 | CVE: CVE-2015-9289_0 | Type: patch\n",
      "     8. CVE-2023-5633_10_patch    | Sim: 0.877 | CVE: CVE-2023-5633_10 | Type: patch\n",
      "     9. CVE-2023-5633_10_vuln     | Sim: 0.877 | CVE: CVE-2023-5633_10 | Type: vuln\n",
      "    10. CVE-2017-16530_1_patch    | Sim: 0.876 | CVE: CVE-2017-16530_1 | Type: patch\n",
      "  Patch rank in results: Not found\n",
      "  Vuln-patch similarity: 1.000\n",
      "  Same CVE precision: 0.000\n",
      "\n",
      "TEST SUMMARY STATISTICS\n",
      "=========================\n",
      "Mean retrieval precision: 0.140\n",
      "Mean vuln-patch similarity: 0.999\n",
      "Patch versions found in top-5: 2/5\n",
      "Mean top-3 similarity: 0.904\n",
      "\n",
      "==================================================\n",
      "\n",
      "KB2 COVERAGE AND QUALITY ANALYSIS\n",
      "===================================\n",
      "Total KB2 entries: 4410\n",
      "Unique CVEs: 2205\n",
      "File type distribution: {'vuln': 2205, 'patch': 2205}\n",
      "Embedding distribution: {'with_embedding': 4410, 'without_embedding': 0}\n",
      "\n",
      "Top 10 most frequent dangerous calls:\n",
      "  kfree: 1022\n",
      "  memcpy: 564\n",
      "  memset: 408\n",
      "  kfree_skb: 165\n",
      "  kmalloc: 153\n",
      "  strcpy: 145\n",
      "  kvfree: 94\n",
      "  sprintf: 86\n",
      "  skb_free_datagram: 64\n",
      "  kmem_cache_free: 42\n",
      "\n",
      "Complexity statistics:\n",
      "  Mean: 7.182\n",
      "  Std: 1.780\n",
      "  Range: [2.167, 20.268]\n",
      "\n",
      "Graph size statistics:\n",
      "  Mean nodes: 194.3\n",
      "  Std nodes: 274.8\n",
      "  Range: [4, 4475]\n",
      "\n",
      "CVEs with both vuln and patch: 2205\n"
     ]
    }
   ],
   "source": [
    "def test_real_cpg_query_pipeline():\n",
    "    \"\"\"\n",
    "    Test complete pipeline with real CPG files to validate search accuracy.\n",
    "    \n",
    "    Scientific validation:\n",
    "        - Query known vulnerable code against KB2\n",
    "        - Measure retrieval precision for vulnerable vs patched versions\n",
    "        - Analyze similarity distributions across different CVE types\n",
    "        - Validate that similar vulnerabilities are correctly identified\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"REAL CPG QUERY PIPELINE TESTING\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Select test CPG files from different CVEs\n",
    "    test_cpg_files = []\n",
    "    cve_patterns = {}\n",
    "    \n",
    "    # Find diverse test cases\n",
    "    for instance_dir in CONFIG['cpg_data_path'].iterdir():\n",
    "        if instance_dir.is_dir():\n",
    "            vuln_file = instance_dir / \"vuln_cpg.json\"\n",
    "            patch_file = instance_dir / \"patch_cpg.json\"\n",
    "            \n",
    "            if vuln_file.exists() and patch_file.exists():\n",
    "                cve_id = instance_dir.name\n",
    "                test_cpg_files.append({\n",
    "                    'cve_id': cve_id,\n",
    "                    'vuln_file': vuln_file,\n",
    "                    'patch_file': patch_file\n",
    "                })\n",
    "                \n",
    "                if len(test_cpg_files) >= 5:  # Test with 5 different CVEs\n",
    "                    break\n",
    "    \n",
    "    print(f\"Testing with {len(test_cpg_files)} CVE instances\")\n",
    "    \n",
    "    test_results = {\n",
    "        'query_accuracy': [],\n",
    "        'vuln_patch_similarity': [],\n",
    "        'cross_cve_similarity': [],\n",
    "        'retrieval_precision': [],\n",
    "        'detailed_results': []\n",
    "    }\n",
    "    \n",
    "    for i, test_case in enumerate(test_cpg_files):\n",
    "        print(f\"\\nTesting CVE {i+1}: {test_case['cve_id']}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        try:\n",
    "            # Process vulnerable version as query\n",
    "            print(\"Processing vulnerable version...\")\n",
    "            with open(test_case['vuln_file']) as f:\n",
    "                vuln_data = json.load(f)\n",
    "            \n",
    "            vuln_graph = graphson_to_networkx(vuln_data)\n",
    "            vuln_embedding = compute_structural_graph_embedding(vuln_graph, CONFIG['embedding_dimensions'])\n",
    "            \n",
    "            # Extract query features\n",
    "            vuln_analysis = analyze_full_cpg(test_case['vuln_file'])\n",
    "            query_features = {\n",
    "                'dangerous_calls': list(vuln_analysis['dangerous_calls'].keys()),\n",
    "                'complexity_score': vuln_graph.number_of_edges() / max(vuln_graph.number_of_nodes(), 1),\n",
    "                'top_calls': list(vuln_analysis['all_calls'].keys())[:10],\n",
    "                'vertex_types': vuln_analysis['vertex_types']\n",
    "            }\n",
    "            \n",
    "            print(f\"  Query graph: {vuln_graph.number_of_nodes()} nodes, {vuln_graph.number_of_edges()} edges\")\n",
    "            print(f\"  Dangerous calls: {query_features['dangerous_calls']}\")\n",
    "            \n",
    "            # Search in KB2\n",
    "            search_results = kb2_search_engine.search_similar_graphs(\n",
    "                vuln_embedding, \n",
    "                query_features, \n",
    "                top_k=10, \n",
    "                hybrid_weight=0.7\n",
    "            )\n",
    "            \n",
    "            # Analyze results\n",
    "            print(f\"  Found {len(search_results)} similar entries\")\n",
    "            \n",
    "            # Check if patch version is in top results\n",
    "            patch_key = f\"{test_case['cve_id']}_patch\"\n",
    "            vuln_key = f\"{test_case['cve_id']}_vuln\"\n",
    "            \n",
    "            patch_rank = None\n",
    "            vuln_rank = None\n",
    "            same_cve_count = 0\n",
    "            \n",
    "            for rank, (entry_key, similarity, metadata) in enumerate(search_results):\n",
    "                if entry_key == patch_key:\n",
    "                    patch_rank = rank + 1\n",
    "                if entry_key == vuln_key:\n",
    "                    vuln_rank = rank + 1\n",
    "                if metadata['cve_id'] == test_case['cve_id']:\n",
    "                    same_cve_count += 1\n",
    "                \n",
    "                print(f\"    {rank+1:2d}. {entry_key:<25} | Sim: {similarity:.3f} | \"\n",
    "                      f\"CVE: {metadata['cve_id']:<15} | Type: {metadata['file_type']}\")\n",
    "            \n",
    "            # Calculate metrics\n",
    "            retrieval_precision = same_cve_count / len(search_results) if search_results else 0\n",
    "            \n",
    "            # Measure vuln-patch similarity specifically\n",
    "            patch_embedding = None\n",
    "            if patch_key in kb2_search_engine.kb2_data:\n",
    "                patch_embedding = np.array(kb2_search_engine.kb2_data[patch_key]['graph_embedding'])\n",
    "                vuln_patch_sim = np.dot(vuln_embedding, patch_embedding)\n",
    "            else:\n",
    "                # Compute patch embedding if not in KB2\n",
    "                with open(test_case['patch_file']) as f:\n",
    "                    patch_data = json.load(f)\n",
    "                patch_graph = graphson_to_networkx(patch_data)\n",
    "                patch_embedding = compute_structural_graph_embedding(patch_graph, CONFIG['embedding_dimensions'])\n",
    "                vuln_patch_sim = np.dot(vuln_embedding, patch_embedding)\n",
    "            \n",
    "            # Store detailed results\n",
    "            test_result = {\n",
    "                'cve_id': test_case['cve_id'],\n",
    "                'query_graph_stats': {\n",
    "                    'nodes': vuln_graph.number_of_nodes(),\n",
    "                    'edges': vuln_graph.number_of_edges(),\n",
    "                    'dangerous_calls_count': len(query_features['dangerous_calls'])\n",
    "                },\n",
    "                'search_results_count': len(search_results),\n",
    "                'patch_rank': patch_rank,\n",
    "                'vuln_rank': vuln_rank,\n",
    "                'same_cve_in_top10': same_cve_count,\n",
    "                'retrieval_precision': retrieval_precision,\n",
    "                'vuln_patch_similarity': float(vuln_patch_sim),\n",
    "                'top_similarities': [float(sim) for _, sim, _ in search_results[:3]]\n",
    "            }\n",
    "            \n",
    "            test_results['detailed_results'].append(test_result)\n",
    "            test_results['retrieval_precision'].append(retrieval_precision)\n",
    "            test_results['vuln_patch_similarity'].append(vuln_patch_sim)\n",
    "            \n",
    "            print(f\"  Patch rank in results: {patch_rank if patch_rank else 'Not found'}\")\n",
    "            print(f\"  Vuln-patch similarity: {vuln_patch_sim:.3f}\")\n",
    "            print(f\"  Same CVE precision: {retrieval_precision:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing {test_case['cve_id']}: {e}\")\n",
    "            logger.error(f\"Error in real CPG testing for {test_case['cve_id']}: {e}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nTEST SUMMARY STATISTICS\")\n",
    "    print(\"=\" * 25)\n",
    "    \n",
    "    if test_results['retrieval_precision']:\n",
    "        mean_precision = np.mean(test_results['retrieval_precision'])\n",
    "        mean_vuln_patch_sim = np.mean(test_results['vuln_patch_similarity'])\n",
    "        \n",
    "        print(f\"Mean retrieval precision: {mean_precision:.3f}\")\n",
    "        print(f\"Mean vuln-patch similarity: {mean_vuln_patch_sim:.3f}\")\n",
    "        \n",
    "        # Count how many times patch was in top-5\n",
    "        patches_in_top5 = sum(1 for r in test_results['detailed_results'] \n",
    "                             if r['patch_rank'] and r['patch_rank'] <= 5)\n",
    "        print(f\"Patch versions found in top-5: {patches_in_top5}/{len(test_results['detailed_results'])}\")\n",
    "        \n",
    "        # Average top similarity scores\n",
    "        all_top_sims = []\n",
    "        for r in test_results['detailed_results']:\n",
    "            all_top_sims.extend(r['top_similarities'])\n",
    "        \n",
    "        if all_top_sims:\n",
    "            print(f\"Mean top-3 similarity: {np.mean(all_top_sims):.3f}\")\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "def analyze_kb2_coverage_and_quality():\n",
    "    \"\"\"\n",
    "    Analyze KB2 dataset coverage and embedding quality.\n",
    "    \n",
    "    Quality metrics:\n",
    "        - CVE distribution analysis\n",
    "        - Embedding norm distribution\n",
    "        - Pairwise similarity analysis\n",
    "        - Feature diversity assessment\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"KB2 COVERAGE AND QUALITY ANALYSIS\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Load KB2 data for analysis\n",
    "    with open(CONFIG['kb2_output_path']) as f:\n",
    "        kb2_data = json.load(f)\n",
    "    \n",
    "    # CVE distribution analysis\n",
    "    cve_counts = {}\n",
    "    file_type_counts = {'vuln': 0, 'patch': 0}\n",
    "    embedding_counts = {'with_embedding': 0, 'without_embedding': 0}\n",
    "    \n",
    "    dangerous_calls_global = Counter()\n",
    "    complexity_scores = []\n",
    "    graph_sizes = []\n",
    "    \n",
    "    for entry_key, entry_data in kb2_data.items():\n",
    "        # Count CVEs\n",
    "        if 'cve_id' in entry_data:\n",
    "            cve_id = entry_data['cve_id']\n",
    "            cve_counts[cve_id] = cve_counts.get(cve_id, 0) + 1\n",
    "        \n",
    "        # Count file types\n",
    "        if 'file_type' in entry_data:\n",
    "            file_type_counts[entry_data['file_type']] += 1\n",
    "        \n",
    "        # Count embeddings\n",
    "        if entry_data.get('embedding_computed', False):\n",
    "            embedding_counts['with_embedding'] += 1\n",
    "            \n",
    "            # Analyze features if available\n",
    "            if 'features' in entry_data:\n",
    "                features = entry_data['features']\n",
    "                \n",
    "                # Dangerous calls\n",
    "                dangerous_calls = features['security_features']['dangerous_calls']\n",
    "                for call, count in dangerous_calls.items():\n",
    "                    dangerous_calls_global[call] += count\n",
    "                \n",
    "                # Complexity\n",
    "                complexity = features['complexity_metrics']['edge_density']\n",
    "                complexity_scores.append(complexity)\n",
    "                \n",
    "                # Graph size\n",
    "                if 'graph_statistics' in entry_data:\n",
    "                    graph_stats = entry_data['graph_statistics']\n",
    "                    graph_sizes.append(graph_stats.get('nodes', 0))\n",
    "        else:\n",
    "            embedding_counts['without_embedding'] += 1\n",
    "    \n",
    "    # Print analysis results\n",
    "    print(f\"Total KB2 entries: {len(kb2_data)}\")\n",
    "    print(f\"Unique CVEs: {len(cve_counts)}\")\n",
    "    print(f\"File type distribution: {file_type_counts}\")\n",
    "    print(f\"Embedding distribution: {embedding_counts}\")\n",
    "    \n",
    "    print(f\"\\nTop 10 most frequent dangerous calls:\")\n",
    "    for call, count in dangerous_calls_global.most_common(10):\n",
    "        print(f\"  {call}: {count}\")\n",
    "    \n",
    "    if complexity_scores:\n",
    "        print(f\"\\nComplexity statistics:\")\n",
    "        print(f\"  Mean: {np.mean(complexity_scores):.3f}\")\n",
    "        print(f\"  Std: {np.std(complexity_scores):.3f}\")\n",
    "        print(f\"  Range: [{np.min(complexity_scores):.3f}, {np.max(complexity_scores):.3f}]\")\n",
    "    \n",
    "    if graph_sizes:\n",
    "        print(f\"\\nGraph size statistics:\")\n",
    "        print(f\"  Mean nodes: {np.mean(graph_sizes):.1f}\")\n",
    "        print(f\"  Std nodes: {np.std(graph_sizes):.1f}\")\n",
    "        print(f\"  Range: [{np.min(graph_sizes)}, {np.max(graph_sizes)}]\")\n",
    "    \n",
    "    # Check for CVEs with both vuln and patch\n",
    "    complete_cves = []\n",
    "    for cve_id, count in cve_counts.items():\n",
    "        if count >= 2:  # Likely has both vuln and patch\n",
    "            complete_cves.append(cve_id)\n",
    "    \n",
    "    print(f\"\\nCVEs with both vuln and patch: {len(complete_cves)}\")\n",
    "    \n",
    "    return {\n",
    "        'total_entries': len(kb2_data),\n",
    "        'unique_cves': len(cve_counts),\n",
    "        'complete_cves': len(complete_cves),\n",
    "        'embedding_coverage': embedding_counts['with_embedding'] / len(kb2_data),\n",
    "        'dangerous_calls_stats': dict(dangerous_calls_global.most_common(20)),\n",
    "        'complexity_stats': {\n",
    "            'mean': float(np.mean(complexity_scores)) if complexity_scores else 0,\n",
    "            'std': float(np.std(complexity_scores)) if complexity_scores else 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Execute comprehensive testing\n",
    "print(\"EXECUTING COMPREHENSIVE KB2 TESTING\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Test real CPG queries\n",
    "real_query_results = test_real_cpg_query_pipeline()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Analyze KB2 quality\n",
    "kb2_quality_analysis = analyze_kb2_coverage_and_quality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "275ef946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINALIZING KB2 CONSTRUCTION\n",
      "===================================\n",
      "GENERATING FINAL KB2 REPORT\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 14:08:09,112 - INFO - KB2 construction and validation completed successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXECUTIVE SUMMARY\n",
      "--------------------\n",
      "KB2 Construction Status: SUCCESS\n",
      "Total entries processed: 4,410\n",
      "Embedding success rate: 100.0%\n",
      "Unique CVEs covered: 2,205\n",
      "Mean retrieval precision: 0.140\n",
      "\n",
      "TECHNICAL SPECIFICATIONS\n",
      "-------------------------\n",
      "Embedding dimension: 128\n",
      "Similarity metric: Cosine similarity + feature-based hybrid\n",
      "Graph representation: NetworkX undirected graphs\n",
      "Feature extraction: Multi-layer structural analysis\n",
      "Storage format: JSON with numpy array serialization\n",
      "\n",
      "PERFORMANCE METRICS\n",
      "--------------------\n",
      "Total processing time: 52.4 seconds\n",
      "Processing rate: 84.2 entries/second\n",
      "KB2 file size: 20.1 MB\n",
      "Memory usage (embeddings): 2.2 MB\n",
      "\n",
      "QUALITY ASSESSMENT\n",
      "--------------------\n",
      "Embedding norm distribution:\n",
      "  Mean: 1.000\n",
      "  Std: 0.000\n",
      "  Range: [1.000, 1.000]\n",
      "Pairwise similarity distribution:\n",
      "  Mean: 0.844\n",
      "  Std: 0.274\n",
      "  Range: [0.108, 1.000]\n",
      "\n",
      "VALIDATION RESULTS\n",
      "--------------------\n",
      "Patch detection rate (top-10): 3/5 (60.0%)\n",
      "Mean vuln-patch similarity: 0.999\n",
      "Retrieval precision: 0.140 ± 0.080\n",
      "\n",
      "RECOMMENDATIONS FOR PRODUCTION\n",
      "--------------------------------\n",
      "✓ KB2 ready for production deployment\n",
      "⚠ Consider tuning similarity thresholds\n",
      "✓ Embedding normalization stable\n",
      "\n",
      "INTEGRATION GUIDELINES\n",
      "-----------------------\n",
      "1. Load KB2 using KB2SimilaritySearchEngine class\n",
      "2. Generate CPG for new code using Joern\n",
      "3. Compute embedding using compute_structural_graph_embedding()\n",
      "4. Search similar graphs with hybrid scoring (recommended weight: 0.7)\n",
      "5. Combine with KB1 textual results for final ranking\n",
      "\n",
      "Detailed report saved: /Users/vernetemmanueladjobi/Documents/RessourcesStages/Projets/VulRAG-Hybrid-System/data/tmp/kb2_construction_report.json\n",
      "\n",
      "==================================================\n",
      "\n",
      "EXPORTING KB2 FOR PRODUCTION\n",
      "==============================\n",
      "Embeddings matrix exported: /Users/vernetemmanueladjobi/Documents/RessourcesStages/Projets/VulRAG-Hybrid-System/data/tmp/kb2_production_export/embeddings_matrix.npy\n",
      "Entry keys mapping exported: /Users/vernetemmanueladjobi/Documents/RessourcesStages/Projets/VulRAG-Hybrid-System/data/tmp/kb2_production_export/entry_keys.json\n",
      "Metadata index exported: /Users/vernetemmanueladjobi/Documents/RessourcesStages/Projets/VulRAG-Hybrid-System/data/tmp/kb2_production_export/metadata_index.json\n",
      "Configuration exported: /Users/vernetemmanueladjobi/Documents/RessourcesStages/Projets/VulRAG-Hybrid-System/data/tmp/kb2_production_export/kb2_config.json\n",
      "Integration example exported: /Users/vernetemmanueladjobi/Documents/RessourcesStages/Projets/VulRAG-Hybrid-System/data/tmp/kb2_production_export/integration_example.py\n",
      "\n",
      "Total export size: 3.6 MB\n",
      "Production export complete: /Users/vernetemmanueladjobi/Documents/RessourcesStages/Projets/VulRAG-Hybrid-System/data/tmp/kb2_production_export\n",
      "\n",
      "KB2 CONSTRUCTION COMPLETE\n",
      "=========================\n",
      "Success rate: 100.0%\n",
      "Total CVEs: 2,205\n",
      "Ready for integration with hybrid RAG system\n"
     ]
    }
   ],
   "source": [
    "def generate_final_kb2_report():\n",
    "    \"\"\"\n",
    "    Generate comprehensive final report for KB2 construction and validation.\n",
    "    \n",
    "    Report includes:\n",
    "        - Processing statistics and performance metrics\n",
    "        - Embedding quality analysis and validation results\n",
    "        - Search engine performance evaluation\n",
    "        - Recommendations for production deployment\n",
    "        - Technical specifications for integration\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"GENERATING FINAL KB2 REPORT\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Collect all analysis results\n",
    "    report_data = {\n",
    "        'system_info': {\n",
    "            'notebook_version': '05_kb2_construction_final',\n",
    "            'generation_date': '2025-06-13',\n",
    "            'joern_cpg_version': 'GraphSON format',\n",
    "            'embedding_method': 'structural_multi_feature',\n",
    "            'embedding_dimensions': CONFIG['embedding_dimensions']\n",
    "        },\n",
    "        'processing_statistics': processing_stats,\n",
    "        'search_engine_stats': kb2_search_engine.get_embedding_statistics(),\n",
    "        'quality_analysis': kb2_quality_analysis,\n",
    "        'query_validation': real_query_results\n",
    "    }\n",
    "    \n",
    "    # Generate summary metrics\n",
    "    summary_metrics = {\n",
    "        'total_kb2_entries': report_data['processing_statistics']['total_entries'],\n",
    "        'successful_embeddings': report_data['processing_statistics']['successful_embeddings'],\n",
    "        'success_rate': report_data['processing_statistics']['successful_embeddings'] / \n",
    "                       report_data['processing_statistics']['total_entries'],\n",
    "        'embedding_coverage': report_data['quality_analysis']['embedding_coverage'],\n",
    "        'unique_cves_count': report_data['quality_analysis']['unique_cves'],\n",
    "        'mean_embedding_norm': report_data['search_engine_stats']['norm_statistics']['mean'],\n",
    "        'mean_retrieval_precision': np.mean(report_data['query_validation']['retrieval_precision']) \n",
    "                                   if report_data['query_validation']['retrieval_precision'] else 0\n",
    "    }\n",
    "    \n",
    "    # Print executive summary\n",
    "    print(\"EXECUTIVE SUMMARY\")\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"KB2 Construction Status: {'SUCCESS' if summary_metrics['success_rate'] > 0.95 else 'PARTIAL'}\")\n",
    "    print(f\"Total entries processed: {summary_metrics['total_kb2_entries']:,}\")\n",
    "    print(f\"Embedding success rate: {summary_metrics['success_rate']:.1%}\")\n",
    "    print(f\"Unique CVEs covered: {summary_metrics['unique_cves_count']:,}\")\n",
    "    print(f\"Mean retrieval precision: {summary_metrics['mean_retrieval_precision']:.3f}\")\n",
    "    \n",
    "    # Technical specifications\n",
    "    print(f\"\\nTECHNICAL SPECIFICATIONS\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"Embedding dimension: {CONFIG['embedding_dimensions']}\")\n",
    "    print(f\"Similarity metric: Cosine similarity + feature-based hybrid\")\n",
    "    print(f\"Graph representation: NetworkX undirected graphs\")\n",
    "    print(f\"Feature extraction: Multi-layer structural analysis\")\n",
    "    print(f\"Storage format: JSON with numpy array serialization\")\n",
    "    \n",
    "    # Performance analysis\n",
    "    processing_time = report_data['processing_statistics']['processing_time']\n",
    "    entries_per_second = summary_metrics['total_kb2_entries'] / processing_time if processing_time > 0 else 0\n",
    "    \n",
    "    print(f\"\\nPERFORMANCE METRICS\")\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Total processing time: {processing_time:.1f} seconds\")\n",
    "    print(f\"Processing rate: {entries_per_second:.1f} entries/second\")\n",
    "    print(f\"KB2 file size: {CONFIG['kb2_output_path'].stat().st_size / (1024*1024):.1f} MB\")\n",
    "    print(f\"Memory usage (embeddings): {report_data['search_engine_stats']['total_embeddings'] * CONFIG['embedding_dimensions'] * 4 / (1024*1024):.1f} MB\")\n",
    "    \n",
    "    # Quality assessment\n",
    "    print(f\"\\nQUALITY ASSESSMENT\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    norm_stats = report_data['search_engine_stats']['norm_statistics']\n",
    "    sim_stats = report_data['search_engine_stats']['similarity_statistics']\n",
    "    \n",
    "    print(f\"Embedding norm distribution:\")\n",
    "    print(f\"  Mean: {norm_stats['mean']:.3f}\")\n",
    "    print(f\"  Std: {norm_stats['std']:.3f}\")\n",
    "    print(f\"  Range: [{norm_stats['min']:.3f}, {norm_stats['max']:.3f}]\")\n",
    "    \n",
    "    print(f\"Pairwise similarity distribution:\")\n",
    "    print(f\"  Mean: {sim_stats['mean_pairwise_similarity']:.3f}\")\n",
    "    print(f\"  Std: {sim_stats['std_pairwise_similarity']:.3f}\")\n",
    "    print(f\"  Range: [{sim_stats['min_pairwise_similarity']:.3f}, {sim_stats['max_pairwise_similarity']:.3f}]\")\n",
    "    \n",
    "    # Validation results\n",
    "    if report_data['query_validation']['detailed_results']:\n",
    "        print(f\"\\nVALIDATION RESULTS\")\n",
    "        print(\"-\" * 20)\n",
    "        \n",
    "        patches_found = sum(1 for r in report_data['query_validation']['detailed_results'] \n",
    "                           if r['patch_rank'] and r['patch_rank'] <= 10)\n",
    "        total_tests = len(report_data['query_validation']['detailed_results'])\n",
    "        \n",
    "        print(f\"Patch detection rate (top-10): {patches_found}/{total_tests} ({patches_found/total_tests:.1%})\")\n",
    "        \n",
    "        mean_vuln_patch_sim = np.mean(report_data['query_validation']['vuln_patch_similarity'])\n",
    "        print(f\"Mean vuln-patch similarity: {mean_vuln_patch_sim:.3f}\")\n",
    "        \n",
    "        precision_scores = report_data['query_validation']['retrieval_precision']\n",
    "        print(f\"Retrieval precision: {np.mean(precision_scores):.3f} ± {np.std(precision_scores):.3f}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\nRECOMMENDATIONS FOR PRODUCTION\")\n",
    "    print(\"-\" * 32)\n",
    "    \n",
    "    if summary_metrics['success_rate'] > 0.95:\n",
    "        print(\"✓ KB2 ready for production deployment\")\n",
    "    else:\n",
    "        print(\"⚠ Review failed embeddings before production\")\n",
    "    \n",
    "    if summary_metrics['mean_retrieval_precision'] > 0.7:\n",
    "        print(\"✓ Search precision suitable for hybrid RAG\")\n",
    "    else:\n",
    "        print(\"⚠ Consider tuning similarity thresholds\")\n",
    "    \n",
    "    if norm_stats['std'] / norm_stats['mean'] < 0.3:\n",
    "        print(\"✓ Embedding normalization stable\")\n",
    "    else:\n",
    "        print(\"⚠ High embedding variance detected\")\n",
    "    \n",
    "    # Integration guidelines\n",
    "    print(f\"\\nINTEGRATION GUIDELINES\")\n",
    "    print(\"-\" * 23)\n",
    "    print(\"1. Load KB2 using KB2SimilaritySearchEngine class\")\n",
    "    print(\"2. Generate CPG for new code using Joern\")\n",
    "    print(\"3. Compute embedding using compute_structural_graph_embedding()\")\n",
    "    print(\"4. Search similar graphs with hybrid scoring (recommended weight: 0.7)\")\n",
    "    print(\"5. Combine with KB1 textual results for final ranking\")\n",
    "    \n",
    "    # Save detailed report\n",
    "    report_path = CONFIG['kb2_output_path'].parent / \"kb2_construction_report.json\"\n",
    "    with open(report_path, 'w') as f:\n",
    "        json.dump(report_data, f, indent=2, default=lambda x: float(x) if isinstance(x, np.floating) else str(x))\n",
    "    \n",
    "    print(f\"\\nDetailed report saved: {report_path}\")\n",
    "    \n",
    "    return report_data, summary_metrics\n",
    "\n",
    "def export_kb2_for_production():\n",
    "    \"\"\"\n",
    "    Export KB2 components for production integration.\n",
    "    \n",
    "    Exports:\n",
    "        - Embedding matrix in efficient format\n",
    "        - Metadata index for fast lookup\n",
    "        - Configuration file for system integration\n",
    "        - Example code for integration\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"EXPORTING KB2 FOR PRODUCTION\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    export_dir = CONFIG['kb2_output_path'].parent / \"kb2_production_export\"\n",
    "    export_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # 1. Export embedding matrix as numpy binary\n",
    "    embeddings_path = export_dir / \"embeddings_matrix.npy\"\n",
    "    np.save(embeddings_path, kb2_search_engine.embeddings_matrix)\n",
    "    print(f\"Embeddings matrix exported: {embeddings_path}\")\n",
    "    \n",
    "    # 2. Export entry keys mapping\n",
    "    keys_path = export_dir / \"entry_keys.json\"\n",
    "    with open(keys_path, 'w') as f:\n",
    "        json.dump(kb2_search_engine.entry_keys, f, indent=2)\n",
    "    print(f\"Entry keys mapping exported: {keys_path}\")\n",
    "    \n",
    "    # 3. Export metadata index\n",
    "    metadata_index = {}\n",
    "    for key, entry in kb2_search_engine.kb2_data.items():\n",
    "        metadata_index[key] = {\n",
    "            'cve_id': entry['cve_id'],\n",
    "            'file_type': entry['file_type'],\n",
    "            'dangerous_calls': list(entry['features']['security_features']['dangerous_calls'].keys()),\n",
    "            'complexity': entry['features']['complexity_metrics']['edge_density'],\n",
    "            'graph_stats': entry.get('graph_statistics', {})\n",
    "        }\n",
    "    \n",
    "    metadata_path = export_dir / \"metadata_index.json\"\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata_index, f, indent=2)\n",
    "    print(f\"Metadata index exported: {metadata_path}\")\n",
    "    \n",
    "    # 4. Export configuration\n",
    "    config_export = {\n",
    "        'embedding_dimensions': CONFIG['embedding_dimensions'],\n",
    "        'similarity_threshold': 0.7,\n",
    "        'hybrid_weight': 0.7,\n",
    "        'top_k_default': 10,\n",
    "        'files': {\n",
    "            'embeddings_matrix': 'embeddings_matrix.npy',\n",
    "            'entry_keys': 'entry_keys.json',\n",
    "            'metadata_index': 'metadata_index.json',\n",
    "            'full_kb2': '../kb2_final_with_embeddings.json'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    config_path = export_dir / \"kb2_config.json\"\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config_export, f, indent=2)\n",
    "    print(f\"Configuration exported: {config_path}\")\n",
    "    \n",
    "    # 5. Export integration example\n",
    "    integration_code = '''\n",
    "\"\"\"\n",
    "KB2 Integration Example\n",
    "Usage example for production RAG system\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "class ProductionKB2Engine:\n",
    "    def __init__(self, export_dir):\n",
    "        # Load precomputed embeddings\n",
    "        self.embeddings = np.load(export_dir / \"embeddings_matrix.npy\")\n",
    "        \n",
    "        with open(export_dir / \"entry_keys.json\") as f:\n",
    "            self.entry_keys = json.load(f)\n",
    "            \n",
    "        with open(export_dir / \"metadata_index.json\") as f:\n",
    "            self.metadata = json.load(f)\n",
    "            \n",
    "        with open(export_dir / \"kb2_config.json\") as f:\n",
    "            self.config = json.load(f)\n",
    "    \n",
    "    def search_similar(self, query_embedding, top_k=10):\n",
    "        # Normalize query\n",
    "        query_norm = query_embedding / np.linalg.norm(query_embedding)\n",
    "        \n",
    "        # Compute similarities\n",
    "        similarities = np.dot(self.embeddings, query_norm)\n",
    "        \n",
    "        # Get top-k\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            entry_key = self.entry_keys[idx]\n",
    "            similarity = float(similarities[idx])\n",
    "            metadata = self.metadata[entry_key]\n",
    "            results.append((entry_key, similarity, metadata))\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Usage:\n",
    "# kb2_engine = ProductionKB2Engine(Path(\"kb2_production_export\"))\n",
    "# results = kb2_engine.search_similar(your_query_embedding)\n",
    "'''\n",
    "    \n",
    "    integration_path = export_dir / \"integration_example.py\"\n",
    "    with open(integration_path, 'w') as f:\n",
    "        f.write(integration_code)\n",
    "    print(f\"Integration example exported: {integration_path}\")\n",
    "    \n",
    "    # Calculate export sizes\n",
    "    total_size = sum(f.stat().st_size for f in export_dir.rglob(\"*\") if f.is_file())\n",
    "    print(f\"\\nTotal export size: {total_size / (1024*1024):.1f} MB\")\n",
    "    \n",
    "    print(f\"Production export complete: {export_dir}\")\n",
    "    \n",
    "    return export_dir\n",
    "\n",
    "# Generate final report and export\n",
    "print(\"FINALIZING KB2 CONSTRUCTION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "final_report, summary = generate_final_kb2_report()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "production_export = export_kb2_for_production()\n",
    "\n",
    "print(f\"\\nKB2 CONSTRUCTION COMPLETE\")\n",
    "print(\"=\" * 25)\n",
    "print(f\"Success rate: {summary['success_rate']:.1%}\")\n",
    "print(f\"Total CVEs: {summary['unique_cves_count']:,}\")\n",
    "print(f\"Ready for integration with hybrid RAG system\")\n",
    "\n",
    "logger.info(\"KB2 construction and validation completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vulrag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
